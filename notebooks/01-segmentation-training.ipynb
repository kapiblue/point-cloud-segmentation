{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping between original labels\n",
    "OG_CLASSES2LABELS = {\n",
    "  0: 0,     # \"unlabeled\"\n",
    "  1: 0,     # \"outlier\" mapped to \"unlabeled\" --------------------------mapped\n",
    "  10: 1,     # \"car\"\n",
    "  11: 2,     # \"bicycle\"\n",
    "  13: 5,     # \"bus\" mapped to \"other-vehicle\" --------------------------mapped\n",
    "  15: 3,     # \"motorcycle\"\n",
    "  16: 5,     # \"on-rails\" mapped to \"other-vehicle\" ---------------------mapped\n",
    "  18: 4,     # \"truck\"\n",
    "  20: 5,     # \"other-vehicle\"\n",
    "  30: 6,     # \"person\"\n",
    "  31: 7,     # \"bicyclist\"\n",
    "  32: 8,     # \"motorcyclist\"\n",
    "  40: 9,     # \"road\"\n",
    "  44: 10,    # \"parking\"\n",
    "  48: 11,    # \"sidewalk\"\n",
    "  49: 12,    # \"other-ground\"\n",
    "  50: 13,    # \"building\"\n",
    "  51: 14,    # \"fence\"\n",
    "  52: 0,    # \"other-structure\" mapped to \"unlabeled\" ------------------mapped\n",
    "  60: 9,     # \"lane-marking\" to \"road\" ---------------------------------mapped\n",
    "  70: 15,    # \"vegetation\"\n",
    "  71: 16,    # \"trunk\"\n",
    "  72: 17,    # \"terrain\"\n",
    "  80: 18,    # \"pole\"\n",
    "  81: 19,    # \"traffic-sign\"\n",
    "  99: 0,     # \"other-object\" to \"unlabeled\" ----------------------------mapped\n",
    "  252: 1,    # \"moving-car\" to \"car\" ------------------------------------mapped\n",
    "  253: 7,    # \"moving-bicyclist\" to \"bicyclist\" ------------------------mapped\n",
    "  254: 6,    # \"moving-person\" to \"person\" ------------------------------mapped\n",
    "  255: 8,    # \"moving-motorcyclist\" to \"motorcyclist\" ------------------mapped\n",
    "  256: 5,    # \"moving-on-rails\" mapped to \"other-vehicle\" --------------mapped\n",
    "  257: 5,    # \"moving-bus\" mapped to \"other-vehicle\" -------------------mapped\n",
    "  258: 4,    # \"moving-truck\" to \"truck\" --------------------------------mapped\n",
    "  259: 5    # \"moving-other\"-vehicle to \"other-vehicle\" ----------------mapped\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS2OG_CLASSES = {\n",
    "  0: 0,    # \"unlabeled\", and others ignored\n",
    "  1: 10,     # \"car\"\n",
    "  2: 11,     # \"bicycle\"\n",
    "  3: 15,     # \"motorcycle\"\n",
    "  4: 18,     # \"truck\"\n",
    "  5: 20,     # \"other-vehicle\"\n",
    "  6: 30,     # \"person\"\n",
    "  7: 31,     # \"bicyclist\"\n",
    "  8: 32,     # \"motorcyclist\"\n",
    "  9: 40,     # \"road\"\n",
    "  10: 44,    # \"parking\"\n",
    "  11: 48,    # \"sidewalk\"\n",
    "  12: 49,    # \"other-ground\"\n",
    "  13: 50,    # \"building\"\n",
    "  14: 51,    # \"fence\"\n",
    "  15: 70,    # \"vegetation\"\n",
    "  16: 71,    # \"trunk\"\n",
    "  17: 72,    # \"terrain\"\n",
    "  18: 80,    # \"pole\"\n",
    "  19: 81    # \"traffic-sign\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticKittiDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, cls_map: dict = OG_CLASSES2LABELS):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.mask_mapping = cls_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_name = os.path.join(self.mask_dir, f\"m{self.images[idx].replace('png', 'npy')[1:]}\")\n",
    "        image = Image.open(img_name)\n",
    "        # there is a general problem with loading depth 16bit image into PIL\n",
    "        # it fixed the issue, but maybe there exist a better solution\n",
    "        image = image.convert(\"I\")\n",
    "        mask = np.loadtxt(mask_name, delimiter=\" \")\n",
    "        # map the original mask into proper ranges\n",
    "        mask = np.vectorize(self.mask_mapping.get)(mask).astype(np.uint8)\n",
    "        sample = {'image': image, 'mask': mask}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformations(object):\n",
    "    \"\"\"Apply Resize and ToTensor transformations.\"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "        resize = transforms.Resize(self.output_size, interpolation=Image.NEAREST)\n",
    "        # image = resize(image)\n",
    "        mask = Image.fromarray(mask)\n",
    "        # mask = resize(mask)\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        mask = np.array(mask, dtype=np.int64)\n",
    "        image /= 65535.0  # normalize 16-bit image\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1]))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'mask': torch.from_numpy(mask)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images are resized into 256x256\n",
    "train_image_dir = 'data/lidar_png/00/'\n",
    "train_mask_dir = 'data/masks/00/'\n",
    "train_dataset = SemanticKittiDataset(image_dir=train_image_dir, mask_dir=train_mask_dir,\n",
    "                                           transform=Transformations(output_size=(256, 256)),\n",
    "                                           cls_map = OG_CLASSES2LABELS)\n",
    "valid_image_dir = 'data/lidar_png/02/'\n",
    "valid_mask_dir = 'data/masks/02/'\n",
    "val_dataset = SemanticKittiDataset(image_dir=valid_image_dir, mask_dir=valid_mask_dir,\n",
    "                                             transform=Transformations(output_size=(256, 256)),\n",
    "                                             cls_map = OG_CLASSES2LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data loaders\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class UNET(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, classes=1):\n",
    "        super(UNET, self).__init__()\n",
    "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
    "\n",
    "        self.double_conv_downs = nn.ModuleList(\n",
    "            [self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
    "\n",
    "        self.up_trans = nn.ModuleList(\n",
    "            [nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2)\n",
    "             for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
    "\n",
    "        self.double_conv_ups = nn.ModuleList(\n",
    "        [self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
    "\n",
    "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def __double_conv(self, in_channels, out_channels):\n",
    "        conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        return conv\n",
    "\n",
    "    def forward(self, x):\n",
    "        # down layers\n",
    "        concat_layers = []\n",
    "\n",
    "        for down in self.double_conv_downs:\n",
    "            x = down(x)\n",
    "            if down != self.double_conv_downs[-1]:\n",
    "                concat_layers.append(x)\n",
    "                x = self.max_pool_2x2(x)\n",
    "\n",
    "        concat_layers = concat_layers[::-1]\n",
    "\n",
    "        # up layers\n",
    "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
    "            x = up_trans(x)\n",
    "            if x.shape != concat_layer.shape:\n",
    "                x = TF.resize(x, concat_layer.shape[2:])\n",
    "\n",
    "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
    "            x = double_conv_up(concatenated)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        # x = torch.nn.functional.softmax(x, dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNET(in_channels=1, classes = 20)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training parameters\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_valid_loss = np.inf\n",
    "history = {'train_losses': [], 'valid_losses': []}\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "model.cuda()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_batch_losses = []\n",
    "    for data in tqdm(train_loader, desc='Training Batches', leave=False):\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        images = data[\"image\"]\n",
    "        labels = data[\"mask\"]\n",
    "        images = images.cuda()\n",
    "        targets = labels.cuda()\n",
    "        outputs = model(images)\n",
    "        # random_output = torch.rand((32, 20, 256, 256), requires_grad = True)\n",
    "        # random_output = random_output.cuda()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_batch_losses.append(loss.item())\n",
    "    train_loss = np.sum(train_batch_losses) / len(train_batch_losses)\n",
    "    history['train_losses'].append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    valid_batch_losses=[]\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images = data[\"image\"]\n",
    "            labels = data[\"mask\"]\n",
    "            images = images.cuda()\n",
    "            targets = labels.cuda()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            valid_batch_losses.append(loss.item())\n",
    "        valid_loss = np.sum(valid_batch_losses) / len(valid_batch_losses)\n",
    "        history['valid_losses'].append(valid_loss)\n",
    "\n",
    "    if min_valid_loss > valid_loss:\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        min_valid_loss = valid_loss\n",
    "\n",
    "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss} \\t\\t Validation Loss: {valid_loss}')\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), 'final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take next sample from data loader\n",
    "sample = next(iter(train_loader))\n",
    "image = sample['image']\n",
    "mask = sample['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(image, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
